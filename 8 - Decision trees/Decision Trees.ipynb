{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://supaerodatascience.github.io/machine-learning/\">https://supaerodatascience.github.io/machine-learning/</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Decision Trees</div>\n",
    "\n",
    "\n",
    "1. [The Iris dataset](#sec1)\n",
    "2. [What is a decision tree?](#sec2)\n",
    "3. [Growing decision trees](#sec3)\n",
    "4. [A note on the Entropy and the Gini index](#sec4)\n",
    "5. [Generalizing the splitting criterion](#sec5)\n",
    "6. [Practicing with trees](#sec6)\n",
    "7. [Variability of decision trees](#sec7)\n",
    "8. [Conclusion on Decision Trees](#sec8)\n",
    "9. [Examples](#sec9)\n",
    "    1. [Spam or ham?](#sec9-1)\n",
    "    2. [NIST](#sec9-2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec1\"></a>1. The Iris dataset\n",
    "- 150 Iris flowers\n",
    "- 3 species (labels)\n",
    "- 4 morphologic features: petal and sepal width and length\n",
    "- 50 examples in each class\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Iris_virginica.jpg/1024px-Iris_virginica.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n",
      "['setosa' 'versicolor' 'virginica']\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "\n",
      " .. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 150 (50 in each of three classes)\n",
      ":Number of Attributes: 4 numeric, predictive attributes and the class\n",
      ":Attribute Information:\n",
      "    - sepal length in cm\n",
      "    - sepal width in cm\n",
      "    - petal length in cm\n",
      "    - petal width in cm\n",
      "    - class:\n",
      "            - Iris-Setosa\n",
      "            - Iris-Versicolour\n",
      "            - Iris-Virginica\n",
      "\n",
      ":Summary Statistics:\n",
      "\n",
      "============== ==== ==== ======= ===== ====================\n",
      "                Min  Max   Mean    SD   Class Correlation\n",
      "============== ==== ==== ======= ===== ====================\n",
      "sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "============== ==== ==== ======= ===== ====================\n",
      "\n",
      ":Missing Attribute Values: None\n",
      ":Class Distribution: 33.3% for each of 3 classes.\n",
      ":Creator: R.A. Fisher\n",
      ":Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      ":Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. dropdown:: References\n",
      "\n",
      "  - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "    Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "    Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "    (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "  - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "    Structure and Classification Rule for Recognition in Partially Exposed\n",
      "    Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "    Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "  - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "    on Information Theory, May 1972, 431-433.\n",
      "  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "    conceptual clustering system finds 3 classes in the data.\n",
      "  - Many, many more ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "print(iris.data.shape)\n",
    "print(iris.target.shape)\n",
    "print(iris.target_names)\n",
    "print(iris.feature_names)\n",
    "print(\"\\n\",iris.DESCR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec2\"></a> 2. What is a decision tree?\n",
    "\n",
    "**Hierarchical description of data based on logical (binary) questions**.\n",
    "\n",
    "Ingredients:\n",
    "- Nodes<br>\n",
    "Each node contains a **test** on the features which **partitions** the data.\n",
    "- Edges<br>\n",
    "The outcome of a node's test leads to one of its child edges.\n",
    "- Leaves<br>\n",
    "A terminal node, or leaf, holds a **decision value** for the output variable.\n",
    "<img src=\"attachment:tree1.png\" width=\"400px\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec3\"></a>3. Growing decision trees\n",
    "\n",
    "Training Set $T$, attributes $x_1,\\ldots, x_p$\n",
    "\n",
    "<div class= \"alert alert-success\">\n",
    "    \n",
    "**FormTree($T$)**<br>\n",
    "<ol>\n",
    "<li> Find best split $(j, s)$ over $T$ // Which criterion?\n",
    "<li> If $(j, s) = \\emptyset$, \n",
    "    <ul>\n",
    "    <li>  node = FormLeaf(T) // Which value for the leaf?\n",
    "    </ul>\n",
    "<li> Else\n",
    "    <ul>\n",
    "    <li> node = $(j, s)$\n",
    "    <li> split $T$ according to $(j, s)$ into $(T1, T2)$\n",
    "    <li> append FormTree($T1$) to node // Recursive call\n",
    "    <li> append FormTree($T2$) to node\n",
    "    </ul>\n",
    "<li> Return node\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "**Classification: splitting criterion**\n",
    "\n",
    "Choose split $(j,s)$ that maximizes the entropy (information) gain:\n",
    "$$N_1 \\sum\\limits_{k=1}^K \\hat{p}_{1k} \\log \\hat{p}_{1k} + N_2 \\sum\\limits_{k=1}^K \\hat{p}_{2k} \\log \\hat{p}_{2k}$$\n",
    "with $N_m$: number of examples in node $m$ and $p_{mk}$: proportion of class $k$ in node $m$\n",
    "\n",
    "Alternate criterion: maximize the Gini index gain.\n",
    "$$N_1 \\sum\\limits_{k=1}^K \\hat{p}_{1k} \\left(1-\\hat{p}_{1k}\\right) + N_2 \\sum\\limits_{k=1}^K \\hat{p}_{2k} \\left( 1- \\hat{p}_{2k} \\right)$$\n",
    "\n",
    "**Classification: value of a leaf node**\n",
    "- majority class.\n",
    "\n",
    "**Regression: splitting criterion**\n",
    "\n",
    "We want to minimize the **sum of squares** error in a leaf node:\n",
    "$$\\sum_{i=1}^{N_m} \\left(y_i - \\hat{f}(x_i)\\right)^2$$\n",
    "Where $\\hat{f}(x_i)$ is the prediction in $x_i$. For simplicity, we take constant node-wise predictions $\\hat{f}(x_i)= \\hat{y}_i$. We introduce the node impurity measure as the mean square error:\n",
    "$$Q_m = \\frac{1}{N_m} \\sum_{i=1}^{N_m} \\left(y_i - \\hat{y}_i\\right)^2 $$\n",
    "The best split is then the one that minimizes\n",
    "$$N_1 Q_1 + N_2 Q_2$$\n",
    "\n",
    "**Regression: value of a leaf node**\n",
    "- average\n",
    "\n",
    "Alternatives (but imply to change the splitting criterion):\n",
    "- median\n",
    "- extreme values\n",
    "- B-spline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec4\"></a> 4. A note on the Entropy and the Gini index\n",
    "\n",
    "Suppose a set of samples $T=\\left\\{(x,y)\\right\\}$ where $y\\in \\{0;1\\}$ and a constant value for $x$. The labels $y$ give some class information on $x$. One relevant question is \"how much useful information is there in $T$?\".\n",
    "\n",
    "- For example, if half of the examples have label \"0\" and the other half has label \"1\", then there is no real useful information in $T$.\n",
    "- On the other hand, if all the examples have the same label, then $T$ is very informative regarding the real class of $x$.\n",
    "\n",
    "Intuitively, a measure of \"disorder\" in a training set is an opposite measure of \"information\". That's what Shannon theorized in his Information theory.\n",
    "\n",
    "A measure of disorder is the (binary) entropy of a set of bits. Suppose $p$ is the probability of class \"1\":\n",
    "\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/f4a7673eba0df7fb924d538fb6b1c94174f812af\" width=\"200px\">\n",
    "\n",
    "$$H(p) = -p \\log_2(p) - (1-p)\\log_2(1-p)$$\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/c9/Binary_entropy_plot.png?20060319144452\" width=\"400px\">\n",
    "\n",
    "When building a tree, we want to choose each split so that it increases the overall information. Hence we want to decrease the overall entropy. Since entropy is additive, if the subscript \"0\" refers of a node and the subscripts \"1\" and \"2\" to its children after a split, we want $N_0 H_0 > N_1 H_1 + N_2 H_2$.\n",
    "\n",
    "The Gini index expresses somehow the same idea. It was introduced by Gini (a sociologist and statistician) in 1936 to describe inequalities of income or wealth within a population. For $K$ classes, it is written:\n",
    "$$\\sum_{k=1}^K p_k(1-p_k)$$\n",
    "\n",
    "To get an intuition of why this is a measure of inequality, suppose a training set $T=\\left\\{(x,y)\\right\\}$ with only two classes $\\{0;1\\}$. Suppose you pick an individuals $x$ at random in $T$ and then you pick a class $y$ at random in $T$ again. What is the probability that $y$ is not the label of $x$?\n",
    "$$p(1-p) + (1-p)(1-(1-p))$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec5\"></a> 5. Generalizing the splitting criterion\n",
    "\n",
    "Overall it is a matter of decreasing an impurity measure in the current node. This impurity measure is the **fitness** criterion our tree tries to (locally) minimize: entropy, Gini index, mean square error."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec6\"></a> 6. Practicing with trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from os import system\n",
    "from IPython.display import Image\n",
    "\n",
    "iris_dt = tree.DecisionTreeClassifier()\n",
    "iris_dt.fit(iris.data, iris.target)\n",
    "\n",
    "tree.plot_tree(iris_dt)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to limit the depth of the tree to preserve the generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dt2 = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth=2)\n",
    "iris_dt2.fit(iris.data, iris.target)\n",
    "tree.plot_tree(iris_dt2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try on 2D complex data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def gen_data(seed):\n",
    "    X1, y1 = datasets.make_gaussian_quantiles(cov=2.,\n",
    "                                 n_samples=300, n_features=2,\n",
    "                                 n_classes=2, random_state=seed)\n",
    "    X2, y2 = datasets.make_gaussian_quantiles(mean=(3, 3), cov=1.5,\n",
    "                                 n_samples=700, n_features=2,\n",
    "                                 n_classes=2, random_state=seed)\n",
    "    X = np.concatenate((X1, X2))\n",
    "    y = np.concatenate((y1, - y2 + 1))\n",
    "    y = 2*y-1\n",
    "    X, y = shuffle(X, y)\n",
    "    return X, y\n",
    "\n",
    "X,y = gen_data(1)\n",
    "Xtest,X = np.split(X,[400])\n",
    "ytest,y = np.split(y,[400])\n",
    "\n",
    "Xblue = X[y==-1]\n",
    "Xred = X[y==1]\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(Xblue[:,0],Xblue[:,1],c='b')\n",
    "_=plt.scatter(Xred[:,0],Xred[:,1],c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change max_depth parameter to 10\n",
    "dt1 = tree.DecisionTreeClassifier(criterion='entropy',max_depth=10)\n",
    "dt1.fit(X,y)\n",
    "\n",
    "\n",
    "tree.plot_tree(dt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary_tree(t, X, y, fig_size=(7,7)):\n",
    "    plot_step = 0.02\n",
    "    x0_min, x0_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x1_min, x1_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx0, xx1 = np.meshgrid(np.arange(x0_min, x0_max, plot_step), np.arange(x1_min, x1_max, plot_step))\n",
    "    yypred = t.predict(np.c_[xx0.ravel(),xx1.ravel()])\n",
    "    yypred = yypred.reshape(xx0.shape)\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.contourf(xx0, xx1, yypred, cmap=plt.cm.Paired)\n",
    "    y_pred = t.predict(X)\n",
    "    Xblue_good = X[np.equal(y,-1)*np.equal(y,y_pred)]\n",
    "    Xblue_bad  = X[np.equal(y,-1)*np.not_equal(y,y_pred)]\n",
    "    Xred_good  = X[np.equal(y,1)*np.equal(y,y_pred)]\n",
    "    Xred_bad   = X[np.equal(y,1)*np.not_equal(y,y_pred)]\n",
    "    plt.scatter(Xblue_good[:,0],Xblue_good[:,1],c='b')\n",
    "    plt.scatter(Xblue_bad[:,0],Xblue_bad[:,1],c='c',marker='v')\n",
    "    plt.scatter(Xred_good[:,0],Xred_good[:,1],c='r')\n",
    "    plt.scatter(Xred_bad[:,0],Xred_bad[:,1],c='m',marker='v')\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary_tree(dt1, X, y)\n",
    "print(\"Training error: %g\"%(1-dt1.score(X,y)))\n",
    "print(\"Generalization error: %g\"%(1-dt1.score(Xtest,ytest)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec7\"></a> 7. Variability of decision trees\n",
    "Let's generate different data sets from the same distribution and compare how different the classification trees are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_size = (7,7)\n",
    "\n",
    "# example 1\n",
    "X,y = gen_data(1)\n",
    "dt1 = tree.DecisionTreeClassifier(criterion='entropy',max_depth=5)\n",
    "dt1.fit(X,y)\n",
    "plot_decision_boundary_tree(dt1, X, y, fig_size)\n",
    "\n",
    "# example 2\n",
    "X,y = gen_data(3)\n",
    "dt1 = tree.DecisionTreeClassifier(criterion='entropy',max_depth=5)\n",
    "dt1.fit(X,y)\n",
    "plot_decision_boundary_tree(dt1, X, y, fig_size)\n",
    "\n",
    "# example 3\n",
    "X,y = gen_data(5)\n",
    "dt1 = tree.DecisionTreeClassifier(criterion='entropy',max_depth=5)\n",
    "dt1.fit(X,y)\n",
    "plot_decision_boundary_tree(dt1, X, y, fig_size)\n",
    "\n",
    "# example 4\n",
    "X,y = gen_data(9)\n",
    "dt1 = tree.DecisionTreeClassifier(criterion='entropy',max_depth=5)\n",
    "dt1.fit(X,y)\n",
    "plot_decision_boundary_tree(dt1, X, y, fig_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec8\"></a> 8. Conclusion on Decision Trees\n",
    "\n",
    "## Summary:\n",
    "\n",
    "Advantages of decision trees:\n",
    "- Simple to read and to interpret, easy to visualize.\n",
    "- Learning the tree has complexity linear in the number of data points.\n",
    "- Forward pass has complexity logarithmic in the number of data points used for training.\n",
    "\n",
    "Drawbacks of decision trees:\n",
    "- Very limited representation power (piecewise constant model).\n",
    "- Very sensitive to overfitting, bad generalization.\n",
    "- **Very dependent on the input data.**\n",
    "- No margin or performance guarantees.\n",
    "   \n",
    "- Often used with ensemble methods (Boosting, Bagging).\n",
    "\n",
    "## Further:\n",
    "- How to prune decision trees?\n",
    "- How to handle missing values?\n",
    "- What about preprocessing data?\n",
    "- What comparison can we make between PCA and trees?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec9\"></a> 9. Examples\n",
    "\n",
    "## <a id=\"sec9-1\"></a>9.1 Spam or ham?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "path.append('../2 - Text data preprocessing')\n",
    "import load_spam\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "spam_data = load_spam.spam_data_loader()\n",
    "spam_data.load_data()\n",
    "\n",
    "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code1.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "spam_dt = tree.DecisionTreeClassifier(criterion='entropy', max_depth=20, min_samples_leaf=10)\n",
    "spam_dt.fit(Xtrain,ytrain)\n",
    "\n",
    "print(\"score:\", spam_dt.score(Xtest,ytest))\n",
    "disp_tree('spam_dt',spam_dt)\n",
    "\n",
    "# Compute cross-validation score\n",
    "nb_trials = 30\n",
    "score = []\n",
    "for i in range(nb_trials):\n",
    "    Xtrain, ytrain, Xtest, ytest = spam_data.shuffle_and_split(2000)\n",
    "    spam_dt = tree.DecisionTreeClassifier(criterion='entropy', max_depth=20, min_samples_leaf=10)\n",
    "    spam_dt.fit(Xtrain,ytrain)\n",
    "    score += [spam_dt.score(Xtest,ytest)]\n",
    "    print('*', end='')\n",
    "print(\" done!\")\n",
    "\n",
    "print(\"Average generalization score:\", np.mean(score))\n",
    "print(\"Standard deviation:\", np.std(score))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look for the best values for `max_depth` and `min_samples_leaf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = np.arange(10,31,2)\n",
    "leaf_size = np.arange(3,16,2)\n",
    "nb_trials = 10\n",
    "scores = np.zeros((depths.shape[0],leaf_size.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for di in range(depths.shape[0]):\n",
    "    for szi in range(leaf_size.shape[0]):\n",
    "        score = []\n",
    "        for i in range(nb_trials):\n",
    "            Xtrain, ytrain, Xtest, ytest = spam_data.shuffle_and_split(2000)\n",
    "            spam_dt = tree.DecisionTreeClassifier(criterion='entropy',\n",
    "                                                  max_depth=depths[di],\n",
    "                                                  min_samples_leaf=leaf_size[szi])\n",
    "            spam_dt.fit(Xtrain,ytrain)\n",
    "            score += [spam_dt.score(Xtest,ytest)]\n",
    "        print('*', end='')\n",
    "        scores[di,szi] = np.mean(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "\n",
    "X, Y = np.meshgrid(leaf_size,depths)\n",
    "\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.gca(projection='3d')\n",
    "surf = ax.plot_surface(X, Y, scores, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "\n",
    "ind = np.unravel_index(np.argmax(scores, axis=None), scores.shape)\n",
    "print(\"maximum score of\", scores[ind], \"obtained at max_depth =\", depths[ind[0]], \"and min_samples_leaf =\", leaf_size[ind[1]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the training on raw word counts, rather than Tf-Idf features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000, feat='wordcount')\n",
    "# Compute cross-validation score\n",
    "nb_trials = 30\n",
    "score = []\n",
    "for i in range(nb_trials):\n",
    "    Xtrain, ytrain, Xtest, ytest = spam_data.shuffle_and_split(2000)\n",
    "    spam_dt = tree.DecisionTreeClassifier(criterion='entropy', max_depth=20, min_samples_leaf=10)\n",
    "    spam_dt.fit(Xtrain,ytrain)\n",
    "    score += [spam_dt.score(Xtest,ytest)]\n",
    "    print('*', end='')\n",
    "print(\" done!\")\n",
    "\n",
    "print(\"Average generalization score:\", np.mean(score))\n",
    "print(\"Standard deviation:\", np.std(score))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec9-2\"></a> 9.2. NIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "#print(digits.data.shape)\n",
    "#print(digits.images.shape)\n",
    "#print(digits.target.shape)\n",
    "#print(digits.target_names)\n",
    "\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "Xtrain,Xtest = np.split(X,[1000])\n",
    "ytrain,ytest = np.split(y,[1000])\n",
    "\n",
    "#print(digits.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def shuffle_and_split(X,y,n):\n",
    "    X0,y0 = shuffle(X,y)\n",
    "    Xtrain,Xtest = np.split(X0,[n])\n",
    "    ytrain,ytest = np.split(y0,[n])\n",
    "    return Xtrain, ytrain, Xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Xtrain.shape)\n",
    "print(ytrain.shape)\n",
    "digits_dt = tree.DecisionTreeClassifier(criterion='entropy', max_depth=20, min_samples_leaf=10)\n",
    "digits_dt.fit(Xtrain,ytrain)\n",
    "prediction = digits_dt.predict(Xtest)\n",
    "#print(\"Training error:\", np.sum(np.not_equal(prediction,ytest))/len(ytest))\n",
    "print(\"Generalization error:\", np.sum(np.not_equal(prediction,ytest))/len(ytest) )\n",
    "print(\"Generalization score:\", digits_dt.score(Xtest,ytest))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec9\"></a> 9.3 Penguins Examples\n",
    "The palmerpenguins data contains size measurements for three penguin species observed on three islands in the Palmer Archipelago, Antarctica.\n",
    "These data were collected from 2007 - 2009 by Dr. Kristen Gorman with the Palmer Station Long Term Ecological Research Program, part of the US Long Term Ecological Research Network. \n",
    "\n",
    "[Download the data](https://www.kaggle.com/parulpandey/palmer-archipelago-antarctica-penguin-data)\n",
    "\n",
    "**Questions**:\n",
    "- Using a scatterplot, highlight the differences between the species on two relevant axis\n",
    "- Using a Decision Tree classification, explain in more details what differentiate the 3 species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"../data/penguins/penguins_size.csv\",encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_sdd_py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "281px",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
